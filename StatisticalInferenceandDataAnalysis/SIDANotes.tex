\documentclass[12pt]{book}
\date{\today}
\title{Statistical Inference and Data Analysis: G0P75b}
\author{Lael John}
\usepackage{amsmath, amsfonts, amssymb, amsthm}


\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{example}{Example}[chapter]
\newtheorem*{huh}{Thoughts}
\newtheorem*{remark}{Remark}
\begin{document}
\maketitle
\chapter*{Preface}
This set of notes deals with SIDA, my second course in statistics and probability. I'm looking to solidify concepts in statistics, and gain some insight into thinking about the correct statistical model(s) to use in a given situation. These notes are also going to be as detailed as I can make them, because they also help in filling in the gaps in my knowledge of probability and statistics. If it seems like I'm stating the obvious, feel free to skip ahead.
\tableofcontents
\chapter{Statistical Models and Estimators}
What are these things? Firstly, statistics deals with stochastic (seemingly random) data, to try and better understand what is happening/why such data was generated in the first place. There's two major tools at a statisticians disposal \begin{itemize}
    \item Probability Theory: Gives us a framework/way of thinking about random phenomena, allows us to study interactions between sets of random events
    \item Random Samples: Gives us a foundation upon which to apply our probability theory knowledge.
\end{itemize}
\section{Probability Theory}
Probability theory must be studied in some "space" so to speak. We deal with classes of events and how those "events" or classes can potentially distort each other. Further, because we deal with such an arbitrary definition of what events are, it becomes necessary to first transport every event to some portion of the REAL NUMBER interval $[0, 1]$, allowing us to then begin working from a numerical foundation.\\
That being said, we denote a probability space as $(\Omega, \mathcal{A}, P)$ where the first element denotes the space of all outcomes, the second element giving us what we will later come to know as a $\sigma$-algebra, and the last giving us a probability function/MEASURE.
\begin{remark}
    Note that when talking about the universe of all outcomes, no mention is made of the fact that those outcomes may or may not be possible. That falls into the realm of our second element, the $\sigma$-algebra. This term seems to deal with a class of subsets, each of which can be measured?
\end{remark}
\begin{huh}
    The concept of a "measure" is one that is slightly abstract. Such a function returns a sort of volume? Or assigns a number to a certain region of the space we're considering, but that number also seems to be determined by a function that's already on that space (like the definition of the Lebesgue measure.)
\end{huh}
\subsection{Random Variables and Vectors}
Now when we talk about a function, $X: \Omega \to \mathbb{R}$, that can be considered a random variable if it is a "measureable" function. Again, take this at face value, but the underlying principle seems to be that elements of a $\sigma$-algebra on(in) our target space are pulled back to elements of  our $\sigma$-algebra in the probability space. \\
NOW, because $X$ is a random variable, mapping the probability space to the set of measurable subsets in $\mathbb{R}$, we can also see that $P$ induces $P_X$ onto the reals, where $P_X(B) = P(X^{-1}(B)) = P(\{\omega \in \Omega| X(\omega) \in B\})$. Here the second of our 3 equivalent expressions shows us the reality of what this induced probability really is. $P$ gives us the measure of the set that maps to $B$ in the target space. The map that gets us to the target space is $X$, so that map that we need to use to get the set in the domain is precisely $X^{-1}$\\
Because we have access to our random variable now, we can begin to talk about other functions that involve the random variable, namely \begin{enumerate}
    \item Cumulative distribution function: $F_X(x) = P(X \leq x) = P(\{\omega \in \Omega | X(\omega) \leq x\})$
    \item Density function: $f_X(x) = \frac{dF_X(x)}{dx} = P(X = x) = P(\{\omega \in \Omega| X(\omega) = x\})$ (Note that this only works if $X$ is absolutely continuous? Will need to review why this is the case.)
    \item Moment Generating Function: A holdover from using mechanics terms within mathematics (curse you Newton), $M_X(t) = E[e^{tX}]$, i.e. the weighted average of all the $e^{tx}$ for all $x$, and a particular $t$. Note also that this function may in fact, not exist at all, but the next one, our \textit{characteristic function} will, for all random variables.
    \item Characteristic Function: Similar to the above except $\phi_X(t) = E[e^{iXt}] = E[cos(Xt)] + iE[sin(Xt)]$
\end{enumerate}
A random vector therefore is basically a collection of random variables from the probability space to the real numbers. Here the probability measure that is introduced becomes the measure of the intersection of the preimages of all the values given by each random variable. Similarly the cumulative distribution function can be defined, though now when we talk about the density function, we take the derivative of the CDF with respect to all our random variables, not just one. (NOTE: some property of higher order derivatives needs to be fulfilled in order for this to happen, needs a little more research). The moment generating function and characteristic functions again, remain the same, with their respective modifications.
\subsection*{Conclusion}
In essence when dealing with probability theory, we have a well defined measure that we can apply onto our universal space, and we study the property of that space, with its sigma algebra all while using our trusty known measure. THIS DOES NOT HAVE TO BE THE CASE WITH MATHEMATICAL STATISTICS.
\section{Mathematical Statistics}
When we begin to talk about mathematical statistics, we begin simply with our universal set of data, and we assume various probability models (or spaces in our previous terminology). Thus our statistical model becomes either one of the following \begin{itemize}
    \item $(\Omega, \mathcal{A}, \{P_{\theta}| \theta \in \Theta\})$
    \item $(\Omega, \mathcal{A}, \{F_{\theta}| \theta \in \Theta\})$ 
\end{itemize}
Here $\Theta$ denotes the parameter space, within which $\theta$ varies, and $P, F$ correspond to various probability measures or density functions respectively.
\subsection{Types of Statistical Models (based on parameter space)}
One way of classifying our statistical models is with the relative size or properties of our parameter space. 
\begin{enumerate}
    \item Parametric statistics: When $\Theta$ has finite dimension (i.e. when it can be thought of as a vector space, each n-dimensional vector corresponding to $n$ different parameters to be used to create the current probability measure in question).
    \item Non-parametric Statistics: Here, stupidly almost $\Theta$ must have an infinite dimension. Therefore it makes no sense for us to even think about specifying a $\theta$. In this case, the different measures correspond to different sequences (countably infinite) or functions (possibly uncountably infinite).
    \item Semi-parametric: What you get when you decide that some parameters can be specified in finite dimensions, but you need to append an infinite dimensional parameter anyway.
\end{enumerate}
\subsection{Statistical Inference}
Inference, or figuring out what $\theta$ seems to have generated the data that we're looking at. Looking at what we've covered already, we're studying a particular value of our statistical model in consideration, knowing that we already have some data to work with. In essence, given a large family of models, we're trying to find a particular parameter that seems closest to reality. (This is a little confusing, given that when we want to talk about reality, we are also using another probability function? even if it's an approximation??? You're basically approximating an approximation, what is happening in the world.)\\
To end up being sucessful in our process of inference, we must necessarily make some estimates about the state of our data. Those estimates (or estimators that generate our estimates) can be classified simply into the following categories:\begin{itemize}
    \item Point Estimators: Where we're trying to find a relatively good approximation of $\theta$.
    \item Confidence intervals: Trying to determine some subset of $\Theta$, that with a reasonably high confidence, we can say $\theta$ lies.
    \item Hypothesis test: Basically partition the space into two, and classify $\theta$ as lying in one part of the space, or its complement.
\end{itemize}
\section{Random Samples}
A random sample, is a collection of \textit{independent} random variables, all having the same distribution as $X$. These are then called individually independent random variables. (Essentially a tuple, each using the same map on the coordinates.) The way one writes a random sample $(X_1, X_2 \ldots X_n)$\\
A \textit{statistic} is now a function from a random sample $(X_1, X_2 \ldots X_n) \to T(X_1, X_2 \ldots X_n)$. Now if we go from an n-dimensional universe of outcomes (n different universes of outcomes) to a simple real number, then $T(X_1, X_2 \ldots X_n)$ is simply a random variable. If however, we go to p-dimensional real space, then each function $T_i(X_1, X_2 \ldots X_n)$ from $i = 1 \ldots p$ is component of $T(X_1, X_2 \ldots X_n)$, where each is a random variable, resulting in a random vector. 
\subsection{Point Estimators}
Now when we consider a statistic $T_n = T(X_1, X_2 \ldots X_n)$ which for the random sample $(X_1, X_2 \ldots X_n)$ in consideration, helps us approximate $\theta$, it becomes known as an \textit{estimator} of $\theta$. A specific value of $T_n$ i.e. $t_n$ is then what gets called an estimate of $\theta$.\\
It may be helpful to consider some examples that help illustrate this concept.
\begin{enumerate}
    \item $\theta = E[X]$ and we consider our estimator to be $\overline{X_n} = \frac{1}{n} \sum_{i=1}^n X_i$, the sample mean.
    \item $\theta = \textnormal{Var}[X] = E[(X - E[X])^2]$ and our estimator to be\\ $S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X_n})^2$, the sample variance.
\end{enumerate}
In general though, one could have multiple estimators for a particular value of $\theta$, which brings us to the question of how we find an estimator that is preferable over others in consideration.

\section{Performance Measures}
Once we do have a statisctical model on a particular random sample, we look to see how our estimators behave given certain performance measures. These could be any of the following \begin{itemize}
    \item \textbf{Bias}: Unsurprisingly, this tells us how accurate our expectation of the estimator is i.e. How far away from the true value of $\theta$ our estimator seems to be. Here $b_{\theta}(T_n) = E_{\theta}[T_n(\mathbf{X})] - \theta$. Here $b_{\theta}$ is our performance measure for each $\theta$, and we look to minimise it. \\ We end up calling our estimator "unbiased" if $b_{\theta}(T_n) = 0 \forall \theta \in \Theta \iff E_{\theta}[T_n(\mathbf{X})] = \theta$.
    \item \textbf{Mean Squared Error}: This is a much more straightforward function, essentially returning the mean of the sum of squared deviations from the actual data. (Squaring allows us to work only with positive values, and look at the cumulative deviation from the sample.). $MSE_{\theta} = E_{\theta}[(T_n(\mathbf{X}) - \theta)^2]$
    \item \textbf{Mean Absolute Deviation Error}: Rather than introducing a squaring term as in the previous measure, we could also just consider the absolute values of all the deviations. The formula for this becomes pretty simple $ABS_{\theta} = E_{\theta}[|T_n(\mathbf{X}) - \theta|]$
    \item \textbf{General Expected Loss/Risk}: Such a function assigns a loss function to each estimator, parameter pair, and the goal then becomes to minimise the expectation of such a function. $R_{\theta}(T_n) = E_{\theta}[L(T_n(\mathbf{X}), \theta)]$
\end{itemize}
\begin{remark}
    Here the goal in all these examples is to minimise the value of the functions. In particular, the general expected risk function allows for the definition of Loss to mirror either the mean absolute deviation, mean squared error. However, it could also set a threshold for acceptable levels of loss, before considering all those models where loss would be higher than that set threshold.
\end{remark}
\subsection{Asymptotic properties of a sequence of statistics}
When considering a sequence of statistics, we try and look at the asymptotic properties of such a sequence. In particular, if $<T_n = T(X_1,X_2\ldots X_n)>$ then when $n \to \infty$ \begin{itemize}
    \item $T_n$ is said to be asymptotically unbiased, if $\textnormal{lt}_{n \to \infty} (b_{\theta}(T_n)) \to 0$. Basically, as $n \to \infty$, the expected value of the estimator tends to the actual value of the parameter.
    \item $T_n$ is weakly consistent, if the estimator in consideration converges in probability to the actual value of our parameter, for all values of $\theta$. Mathematically, $$\forall \theta \in \Theta, T_n \to^P \theta, n \to \infty$$ \begin{remark}
        Here note that convergence in probabilty simply states that for a given value of $\epsilon$, the probability of a member of a sequence of random variables differs at most by $\epsilon$ from some fixed random variable, as $n \to \infty$
    \end{remark}
    \item $T_n$ is strongly consistent if $$\forall \theta \in \Theta, T_n \to^{a.s} \theta, n \to \infty$$ \begin{remark}
        Almost sure convergence is defined by the fact $$P(\{s \in S | \lim_{n \to \infty} X_n(s) = X(s)\}) = 1$$ where $S$ is the sample space. The probability of, for each outcome, the sequence of random variables tending to a fixed random variable, is exactly 1. This chain implies weak consistency.  
    \end{remark}
    \item Finally, $T_n$ can be said to be Mean Square Consistent, if the Mean Square Error (for an arbitrary value of $\theta$) of $T_n$ tends to 0 as $n \to \infty$.
\end{itemize}
\subsection{Asymptotic Normality}
We're now looking at the behaviour of certain sequences of statistics, and seeing if they resemble normal distributions as $n \to \infty$. What fresh hell is this.
\begin{definition}
    A univariate estimator $T_n$ for $\theta \in \Theta$ is said to be \textit{asymptotically normally distributed} if, $\forall \theta \in \Theta, \exists V_{\theta} > 0$ such that $$\sqrt{n}(T_n - \theta) \to^D N(0, V_{\theta}), n \to \infty$$
    Here $V_{\theta}$ is called the asymptotic variance of $T_n$, and $T_n \approx N(\theta, \frac{V_{\theta}}{n})$
\end{definition}
\begin{remark}
    Here to understand such a definition, when trying to estimate a point value for our parameter, we check the difference between the estimator and the sample value as we go further along the sequence. As that starts to resemble a normal distribution with mean 0 and some positive variance, the defnition is satisfied.
    The symbol symbolises a convergence in distribution.
\end{remark}
\begin{remark}
    We can extend such a definition to multivariate estimators, where now instead of considering just one $V_{\theta}$, instead we look for a positive-definite, symmetric matrix $\Sigma_{\theta}$, called the asymptotic variance-covariance matrix as $n \to \infty$ if it exists.
\end{remark}

If $T_n$ is asymptotically normal, then we consider $\sqrt{n}(T_n - \theta)$ to be bounded in probability.

\subsection{Functions of estimators}
Now that we have a basic understanding of estimators and how to measure their performance and long term behaviour, we can begin to talk about functions on given estimators. Very simply put, if $T_n$ is an estimator of $\theta$, and $g$ is a function, then $g(T_n)$ is an estimator of $g(\theta)$.

The \textit{Delta Method} tells us that given an asymptotically normal univariate estimator for the parameter, and $g$ being differentiable on $\mathbb{R}$, never $0$, then $$\sqrt{n}(g(T_n) - g(\theta)) \to^D N(0, g'(\theta)^2 V_{\theta})$$

\begin{remark}
    Note though, that $V_{\theta}$ does seem to depend on $\theta$ for an asymptotically normal estimator. Now the question arises wheter we can find an estimator for which this is not the case, i.e if there exists a function $g$ such that when applied on our estimator, the resulting estimator tends in distribution towards a normal distribution around 0, with a constant positive variation, for all $\theta$. Clearly this means $g'(\theta)^2 V_{\theta} = c^2$ which boils down to solving the differential equation $\frac{d g(\theta)}{d\theta} = \frac{c}{\sqrt{V_{\theta}}}$
\end{remark}

In similar fashion, we can also talk about a function of a multivariate estimator being asymptotically normal, and a similar remark also follows.

\section{Optimal Estimators}
When considering a model $(\Omega, \mathcal{A}, \{F_{\theta}| \theta \in \Theta \})$, then how does one begin to find a 'best' estimator for $\theta$, based on a random sample given? In general the answer would be to either remove bias entirely, or minimise one of the performance measures that deals with error. In particular, this might also be a minimisation of our general expected loss/risk function.

Such a minimisation may not always be possible, hence our restricting the class of estimators in consideration, to better arrive at an answer.
\subsection{UMVUE (Uniform Minimum Variance Unbiased Estimator)}
When considering the class of unbiased estimators, with a finite variance, then an estimator is said to be an UMVUE of $\theta$ if \begin{itemize}
    \item $E_{\theta}[T_n] = \theta \forall \theta \in \Theta$
    \item For any other $S_n$ unbiased estimator of $\theta$, $\textnormal{Var}_{\theta}[T_n] \leq \textnormal{Var}_{\theta}[S_n]$
\end{itemize}
A characterisation of such an UMVUE given by CR Rao tells us that if we consider an unbiased estimator with finite variance, for all parameter values, then it is an UMVUE $\iff E_{\theta}[T_nU_n] = 0 \forall \theta \in \Theta$, and $\forall U_n$, an unbiased estimator of 0 with finite variance.

An UMVUE is unique (slightly obvious)

\subsection{BLUE (Best Linear Unbiased Estimators)}
Sometimes when considering classes of estimators, we can restrict our set even further to just those estimators that are unbiased, and a linear combination of the observations. $T_n$ is a BLUE if \begin{itemize}
    \item $E_{\theta}[T_n] = \theta \forall \theta \in \Theta$
    \item $T_n$ is a linear estimator ($T_n = \sum_{i = 1}^n c_iX_i, c_i \in \mathbb{R})$
    \item For any other linear unbiased estimator, the variance of $T_n$ is lesser or equal to it, for all values of $\theta$.
\end{itemize}
\begin{remark}
    UMVUE $\implies$ BLUE. Also, there do exist many distributions where it may not be possible to find linear unbiased estimators at all. 
\end{remark}
\subsection{Minimax Estimators}
We could also search for the estimators whose maximal possible risk is the least. Such an estimator becomes a Minimax Estimator.
\subsection{Bayesian Estimator}
In the Bayesian framework, here our parameters are also random variables/vectors, distributed over the space $\Theta$. First a prior distribution is assumed (based on past data/current beliefs, but an assumption is made regardless.) The information of the random sample is combined with the prior distribution to better estimate $\theta$.
\end{document}